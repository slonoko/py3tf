import nltk
import numpy as np
import os
import shutil
import tensorflow as tf

nltk.download("treebank")
data_dir = "/home/elie/nltk_data"

def download_and_read(dataset_dir, num_pairs=None):
   sent_filename = os.path.join(dataset_dir, "treebank-sents.txt")
   poss_filename = os.path.join(dataset_dir, "treebank-poss.txt")
   if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):
       if not os.path.exists(dataset_dir):
           os.makedirs(dataset_dir)
       fsents = open(sent_filename, "w")
       fposs = open(poss_filename, "w")
       sentences = nltk.corpus.treebank.tagged_sents()
       for sent in sentences:
           fsents.write(" ".join([w for w, p in sent]) + "\n")
           fposs.write(" ".join([p for w, p in sent]) + "\n")
       fsents.close()
       fposs.close()
   sents, poss = [], []
   with open(sent_filename, "r") as fsent:
       for idx, line in enumerate(fsent):
           sents.append(line.strip())
           if num_pairs is not None and idx >= num_pairs:
               break
   with open(poss_filename, "r") as fposs:
       for idx, line in enumerate(fposs):
           poss.append(line.strip())
           if num_pairs is not None and idx >= num_pairs:
               break
   return sents, poss
sents, poss = download_and_read("./datasets")
assert(len(sents) == len(poss))
print("# of records: {:d}".format(len(sents)))

def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):
   if vocab_size is None:
       tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)
   else:
       tokenizer = tf.keras.preprocessing.text.Tokenizer(
           num_words=vocab_size+1, oov_token="UNK", lower=lower)
   tokenizer.fit_on_texts(texts)
   if vocab_size is not None:
       # additional workaround, see issue 8092
       # https://github.com/keras-team/keras/issues/8092
       tokenizer.word_index = {e:i for e, i in
           tokenizer.word_index.items() if 
           i <= vocab_size+1 }
   word2idx = tokenizer.word_index
   idx2word = {v:k for k, v in word2idx.items()}
   return word2idx, idx2word, tokenizer
word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(
   sents, vocab_size=9000)
word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(
   poss, vocab_size=38, lower=False)
source_vocab_size = len(word2idx_s)
target_vocab_size = len(word2idx_t)
print("vocab sizes (source): {:d}, (target): {:d}".format(
   source_vocab_size, target_vocab_size))

sequence_lengths = np.array([len(s.split()) for s in sents])
print([(p, np.percentile(sequence_lengths, p))
   for p in [75, 80, 90, 95, 99, 100]])

max_seqlen = 271
sents_as_ints = tokenizer_s.texts_to_sequences(sents)
sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(
   sents_as_ints, maxlen=max_seqlen, padding="post")
poss_as_ints = tokenizer_t.texts_to_sequences(poss)
poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(
   poss_as_ints, maxlen=max_seqlen, padding="post")
poss_as_catints = []
for p in poss_as_ints:
   poss_as_catints.append(tf.keras.utils.to_categorical(p,
       num_classes=target_vocab_size+1, dtype="int32"))
poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(
   poss_as_catints, maxlen=max_seqlen)
dataset = tf.data.Dataset.from_tensor_slices(
   (sents_as_ints, poss_as_catints))
idx2word_s[0], idx2word_t[0] = "PAD", "PAD"
# split into training, validation, and test datasets
dataset = dataset.shuffle(10000)
test_size = len(sents) // 3
val_size = (len(sents) - test_size) // 10
test_dataset = dataset.take(test_size)
val_dataset = dataset.skip(test_size).take(val_size)
train_dataset = dataset.skip(test_size + val_size)
# create batches
batch_size = 128
train_dataset = train_dataset.batch(batch_size)
val_dataset = val_dataset.batch(batch_size)
test_dataset = test_dataset.batch(batch_size)

class POSTaggingModel(tf.keras.Model):
   def __init__(self, source_vocab_size, target_vocab_size,
           embedding_dim, max_seqlen, rnn_output_dim, **kwargs):
       super(POSTaggingModel, self).__init__(**kwargs)
       self.embed = tf.keras.layers.Embedding(
           source_vocab_size, embedding_dim, input_length=max_seqlen)
       self.dropout = tf.keras.layers.SpatialDropout1D(0.2)
       self.rnn = tf.keras.layers.Bidirectional(
           tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))
       self.dense = tf.keras.layers.TimeDistributed(
           tf.keras.layers.Dense(target_vocab_size))
       self.activation = tf.keras.layers.Activation("softmax")
   def call(self, x):
       x = self.embed(x)
       x = self.dropout(x)
       x = self.rnn(x)
       x = self.dense(x)
       x = self.activation(x)
       return x
embedding_dim = 128
rnn_output_dim = 256
model = POSTaggingModel(source_vocab_size, target_vocab_size,
   embedding_dim, max_seqlen, rnn_output_dim)
model.build(input_shape=(batch_size, max_seqlen))
model.summary()
model.compile(
   loss="categorical_crossentropy",
   optimizer="adam",
   metrics=["accuracy", masked_accuracy()])

def masked_accuracy():
   def masked_accuracy_fn(ytrue, ypred):
       ytrue = tf.keras.backend.argmax(ytrue, axis=-1)
       ypred = tf.keras.backend.argmax(ypred, axis=-1)
       mask = tf.keras.backend.cast(
           tf.keras.backend.not_equal(ypred, 0), tf.int32)
       matches = tf.keras.backend.cast(
           tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask
       numer = tf.keras.backend.sum(matches)
       denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)
       accuracy =  numer / denom
       return accuracy
   return masked_accuracy_fn

num_epochs = 50
best_model_file = os.path.join(data_dir, "best_model.h5")
checkpoint = tf.keras.callbacks.ModelCheckpoint(
   best_model_file,
   save_weights_only=True,
   save_best_only=True)
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)
history = model.fit(train_dataset,
   epochs=num_epochs,
   validation_data=val_dataset,
   callbacks=[checkpoint, tensorboard])

